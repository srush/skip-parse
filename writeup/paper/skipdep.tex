\documentclass[11pt,a4paper]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{proof}
\usepackage{caption}
\usepackage{subcaption}
\usetikzlibrary{arrows}
\usepackage{algpseudocode}
\usetikzlibrary{decorations.markings}
\tikzset{
  >=latex,text height=1.5ex,text depth=0.25ex
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Actually apply letter size for PDFs; letterpaper just scales text -K
\setlength{\pdfpagewidth}{8.268in}
\setlength{\pdfpageheight}{11.693in}

%\setlength\titlebox{5cm}

\algtext*{EndFor}% Remove "end while" text
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text
\algtext*{EndProcedure}% Remove "end procedure" text

\newenvironment{packed_enumerate}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}
\newenvironment{packed_itemize}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newcommand{\NULL}{\ensuremath{\epsilon}}
\newcommand{\Enum}[1]{\{1 \ldots #1\}}
\newcommand{\EnumS}[2]{\{#1 \ldots #2\}}
\newcommand{\Set}[1]{\big\{ #1 \big\}}
\newcommand{\Bin}[1]{\{0,1\}^{#1}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\air}{\vspace{0.25cm}}

\newcommand{\IndexSet}{{\cal I}}
\newcommand{\IndexSetB}{{\cal J}}

% Commands for drawing the dependency glyphs.

\newcommand{\abox}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (0,1.5cm);
    \coordinate (C) at (-2cm, 1.5cm);
    \coordinate (D) at (-2cm, 0cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}

\newcommand{\rtrap}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (2.5,0.9cm);
    \coordinate (D) at (0:2.5cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}


\newcommand{\ltrap}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (-2.5,0.9cm);
    \coordinate (D) at (180:2.5cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
    }}}

% \newcommand{\rtrapskip}{\scalebox{0.2}{\tikz{
%     \coordinate (A) at (0,0);
%     \coordinate (B) at (90:1.5cm);
%     \coordinate (C) at (2.5,0.9cm);
%     \coordinate (D) at (0:2.5cm);
%     \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
%     }}}

\newcommand{\rtrapskip}{\scalebox{0.2}{\tikz{
      \begin{scope}[decoration={
          markings,
          mark=at position 0.3 with {\arrow[scale=2.75]{<}},
          mark=at position 0.55 with {\arrow[scale=2.75]{<}},
          mark=at position 0.8 with {\arrow[scale=2.75]{<}}
        }]
        \coordinate (A) at (0,0);
        \coordinate (B) at (90:1.5cm);
        \coordinate (C) at (2.5,0.9cm);
        \coordinate (D) at (0:2.5cm);
        \draw[postaction={decorate}, line width = 0.05cm] (B) -- (C);
        \draw[line width = 0.05cm] (A)--(B)--(C)--(D)--cycle;
      \end{scope}
    }}}

\newcommand{\rtriskip}{\scalebox{0.2}{\tikz{
      \begin{scope}[decoration={
          markings,
          %mark=at position 0.3 with {\arrow[scale=2.75]{|}},
          %mark=at position 0.55 with {\arrow[scale=2.75]{|}},
          mark=at position 0.7 with {\arrow[scale=2.75]{|}}
        }]
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:-1.7cm);
        \draw[postaction={decorate}, line width = 0.05cm] (B) -- (C);
        \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
      \end{scope}
    }}}

\newcommand{\rtri}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:-1.7cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
    }}}

\newcommand{\ltri}{\scalebox{0.2}{\tikz{
    \coordinate (A) at (0,0);
    \coordinate (B) at (90:1.5cm);
    \coordinate (C) at (180:1.7cm);
    \draw[line width = 0.05cm] (A)--(B)--(C)--cycle;
    }}}


\title{Efficient Multi-Structure Sentence Compression}
\title{Parsing with Skips for Sentence Compression}
\title{Optimal and Efficient Multi-Structure Compression}
\title{Compressive Parsing}
\title{First and Second-Order Compressive Parsing}
\title{Efficient Compressive Parsing for Sentence Compression}

\title{Sentence Compression as Dependency Parsing}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
    Supervised approaches to sentence compression have previously been shown
    to benefit when the dependency structure of the output sentence is
    included in the inference objective, but prior formulations of this
    inference task do not admit efficient algorithms.
%    and require integer programming or approximate solutions.
    We present a polynomial-time approach to recover dependency trees
    for compressed sentences in which dynamic programming is used to
    produce a valid parse tree over a subset of the tokens in the input
    in order to satisfy a compression rate. In addition to first-order
    dependency structure, this approach also recovers
    second-order dependencies as well as a bigram factorization of the
    output sentence with no asymptotic overhead.
    Experiments on two standard corpora for sentence compression show
    improved performance against previous work

\end{abstract}

\section{Introduction}
\label{intro}

Text-to-text generation problems such as paraphrase generation,
sentence fusion and text simplification involve
monolingual transformations of text for specific goals
such as reducing verbosity or assisting comprehension.
The most popular of these problems is the task of
sentence \emph{compression} which involves
the production of well-formed output sentences that are shorter than
their corresponding input sentences while nevertheless preserving their
meaning.
Compression problems have received significant attention in
recent years due to their usefulness in document summarization~\cite{zajic07}
as well as the increasing number of sources of compression
data~\cite{knight00,clarke06a,galanis11,filippova13} for training
and evaluation.

%While sentence compression is often
%incorporated into systems for document
%summarization~\cite{daume02,zajic07,clarke07,martins09a,bergkirkpatrick11,woodsend12,almeida13,molina13,li13,qian13}, the standalone compression task
%has also received significant attention,
%in part due to the availability of parallel sentence corpora such as
%the Ziff-Davis corpus~\cite{knight00} and the Edinburgh compression
%corpora~\cite{clarke06a}.

Sentence compression is usually formulated as a \textit{token deletion}
problem\footnote{Following the terminology used for text
    summarization, this setting is also referred to as \emph{extractive}
    compression by \newcite{cohn08a} and \newcite{galanis10}.}
in which the compressed output is synthesized using only the
tokens in the input sentence without any reordering or paraphrasing,
as seen in the following example from the corpus of
\newcite{clarke06a}.

\begin{tabular}{p{190pt}}
    \small\vspace{0pt}
\textbf{Original:} In 1967 Chapman, who had cultivated a
    conventional image with his ubiquitous tweed jacket and pipe, by his own
    later admission stunned a party attended by his friends and future Python
    colleagues by coming out as a homosexual.\\[4pt]
        \small
\textbf{Compressed:} In 1967 Chapman, who had cultivated a
conventional image, stunned a party by coming out as a homosexual.\\[4pt]
\end{tabular}
%Compressed sentences are typically generated by inference algorithms
%that target some structured representation of output sentences.
While a wide variety of structured inference techniques have been
proposed to
assemble compressed sentences in this setting, the production of
compressed dependency trees is prevalent in most recent work for
standalone sentence
compression~\cite{filippova08a,nomoto09,galanis10,thadani13a,filippova13,thadani14}
as well as joint compression and
summarization~\cite{martins09a,almeida13,qian13}. However, each of these
approaches is limited by one or more of the following: (i) a restriction
that the output tree can only use edges from the input parse,
resulting in unreachable gold compressions and increased sensitivity to
parser errors,
(ii) an intractable formulation which necessitates the use of expensive
integer linear programs (ILPs) or algorithms for approximate solutions,
and (iii) a reliance on hand-crafted rules or constraints which can be
brittle and domain-dependent.

In this work, we present polynomial-time algorithms for
\emph{compressive parsing} which generate optimal dependency trees
from all possible parse trees over output compressions without any need for
heuristics or hand-crafted rules.
These dynamic programming strategies extend the well-known Eisner
algorithm for projective parsing~\cite{eisner96}
in order to drop a fixed or variable number of tokens
in the output parse tree.
Our formulation also scores a bigram
factorization of the compressed sentence with no asymptotic overhead,
thereby offering an efficient alternative to the ILP for joint n-gram and
dependency structured inference proposed by \newcite{thadani13a}.
%\footnote{
%    \newcite{thadani13a} produce \emph{non-projective} trees which cannot
%    be recovered efficiently~\cite{lau06} and must
%    consequently rely on ILPs or LP relaxations~\cite{thadani14}.}
Finally, this dynamic program can easily be extended to richer
second-order compressive parsing \cite{eisner96,mcdonald06b} in
which scores can be defined over consecutive parsing decisions without
any increase in runtime.

The contributions of this paper include:
\begin{packed_itemize}
\item An $O(n^3)$ time dynamic programming algorithm to jointly recover the
    optimal compressed dependency tree and bigram factorization over an
    input sentence of length $n$ when no compression rate is specified.
\item An $O(n^3m^2)$ time algorithm to recover the optimal compressed
    tree and bigram sequence covering exactly $m < n$ tokens.
\item A bisection-based first-pass strategy to decrease runtime
    by recovering a fraction of compressions with $m$ tokens in
    $O(n^3k)$ where $k << m^2$.
\end{packed_itemize}

\section{Parsing for Compression Inference}
\label{parsing}

A diverse array of inference strategies have been employed in recent
years towards standalone sentence compression and compression within
extractive summarization. In particular, the production of
arc-factored dependency trees to represent compressed sentences
has gained appeal by balancing the straightforwardness
of inference approaches for n-gram subsequences~\cite{mcdonald06a,clarke08}
and the syntactic perspective of tree substitution
grammars~\cite{cohn09,woodsend12} while performing competitively in
experimental evaluations~\cite{galanis10,qian13,thadani14}.

Many recent techniques produce compressed dependency trees by
pruning edges from a dependency parse of the input
sentence or transformations thereof. However, this problem is NP-hard
in the general
case\footnote{A polynomial-time algorithm is viable when the edge weights
    are integers~\cite{lau06}.}
and has consequently been addressed with expensive integer linear
programs~\cite{filippova08a,filippova13} or approximations based on
reranking~\cite{nomoto09,galanis10}. Furthermore, a strong reliance
on the input parse makes these techniques sensitive to parse errors
as well as prone to unreachable reference compressions.

Further constraining the head of the output tree to match that of the
input permits only subtrees to be dropped from the input parse; in this case,
the best compressed tree can be found in linear time using the Viterbi
algorithm for trees. This approach is favored by joint compression
and summarization
systems~\cite{martins09a,gillick09,bergkirkpatrick11,almeida13,qian13}
but the head restriction further exacerbates the fraction of unreachable
reference sentences in a standalone sentence compression setting.
Many of these approaches also define hand-crafted rules
to ensure that important subtrees like the subject of the head aren't
removed.

The joint n-gram and dependency compression approach of
\newcite{thadani13a} does not restrict output parse trees to lie
within input parses but inference remains NP-hard~\cite{thadani14}
because the objective is defined over non-projective trees.
However, since tokens cannot be reordered in extractive sentence
compression, the recovery of linguistically-motivated\footnote{English
    is mostly projective and even
    canonical non-projective languages such
    as Czech, Danish and Turkish have a low rate (1-2\%) of
    non-projective arcs in their respective treebanks~\cite{nivre05}.}
\emph{projective} trees becomes tractable. With the exception of
non-projectivity, the dynamic programming approach described in
the following section fully generalizes over these joint models.

The rest of this section is organized as follows:



\section{Related Work}


\cite{eisner99dp}

\cite{huang2005machine}

\section{Background}
We begin by presenting formal background for sentence compression. Given an input sentence as a sequence $s_1 \ldots s_n$, a sentence compression is an ordered sub-sequence of tokens, i.e. a sequence $s_{p_1} \ldots s_{p_m}$ for some
$1 \leq p_1 < \ldots < p_m \leq n$. 

Instead of directly predicting the sub-sequence $p$, we model compression as a joint parsing and language modeling problem. We first give notation for these models and then describe their use for compression.

% Our
% system models compression as a joint parsing and language modeling
% problem. We first describe the specifics of these techniques and then
% their combined use.

% Given an input sentence $s_1 \ldots s_n$, define the set of feasible
% compressions as all ordered subsets, i.e. a compression is $s_{p_1} \ldots s_{p_m}$ for a sequence $p$ with
% $1 \leq p_1 < \ldots < p_m \leq n$ and $m < n$. 

% Our aim is the find the highest-scoring compression under a given
% objective.

To incorporate a simple language model over sub-sequences, we define the index set ${\cal I}$ of possible compression bigrams 

\[ \IndexSet = \Set{(i,j) : 0 \leq i < j \leq n+1} \]

\noindent The element $(i,j)$ is used to indicate that $p_k = i$ and $p_{k+1}
=j$ for some index $0 \leq k \leq m$ where for boundary cases define terms $p_0$ and $p_{m+1}$ as the start/end symbols $\texttt{<s>}$ and $\texttt{</s>}$ respectively.

The set of valid compression bigram sequences ${\cal Y}$ is defined as
 
\begin{equation*}
  {\cal Y} = \begin{cases}\displaystyle y \in \Bin{\IndexSet} : 
    \sum_{i=1}^{n+1} y_{0,i} = 1,\ \sum_{i=0}^n y_{i, n+1} = 1, \\\\
    \displaystyle\ \ \ \ \ \ \
    \sum_{i=0}^{j-1} y_{ij} = \sum_{k=j+1}^{n+1} y_{jk}\  \forall\ 1\leq j \leq n  \end{cases}
\end{equation*}

\noindent And the score of each bigram is given by a score vector $\theta \in \Reals^{\IndexSet}$.



% The aim of the problem is to find the highest-scoring compression under 
% an objective function $f$  


% Our aim is to simultaneously maximize

Next, to incorporate syntactic structure, define the index set of dependency arcs over the
original sentence as
\[ \IndexSetB = \Set{(i,j) : \forall \  i \in \EnumS{0}{n}, j \in \Enum{n} } \]
\noindent where the arc $(i,j)$ will indicate that $s_i$ is the head word of $s_j$, and $i=0$ is a 
special \textit{root} position $*$.    

The set of \textit{compressed} dependency parses is ${\cal Z}'$ where
\[{\cal Z }' = \{z \in \Bin{\IndexSetB}: z \hbox{ is a directed tree rooted at *} \}  \]

\noindent 
The score of a dependency arc is given by a scoring vector $\omega \in
\Reals^{\IndexSetB}$.


Note that standard dependency parsing requires $z$ to be a directed
\textit{spanning} tree, i.e. $||z||_1 = n$. For compression
we allow $0 \leq ||z||_1 \leq n$. Compression rate is discussed in 
Section~\ref{sec:comprate}.   


% . For a parse in $y \in {Y}$ define the span of $y$ as
% $||z||_1 = \sum_{(i, j) \in \IndexSetB} z_{ij}$. Standard dependency
% parsing requires the span of a parse to be $n$.

Finally define the score of a sentence compression is the
sum of the score of the compressed dependency parse and the
corresponding language model score. The decoding problem is to find
\begin{eqnarray*}
 \argmax_{y \in {\cal Y}, z\in {\cal Z}'} &&\theta^\top y +  \omega^\top z \\
\mathrm{s.t.} &&  \sum_{i = 0}^{j-1} y_{i j} =  \sum_{i=0}^n z_{i j} \ \ \forall\  1 \leq j \leq n 
\end{eqnarray*}
\noindent The compressed sub-sequence $p$ can be directly read from $y$. 

Unfortunately, solving this maximization problem has been shown to be NP-hard \cite{}. This is mainly because of the difficulty of constrained maximum directed spanning tree problems.  

In this paper we focus on a variant form of dependency parsing known
as \textit{projective} parsing. Informally, a projective parse requires that for any $z_{i,j}=1$ and
$z_{i'j'}=1$ the arcs do not cross each other. Define the set ${\cal Z}$ as all compressive projective parses. 
The projective assumption allows us to use dynamic programming for decoding and to vastly improve worst-case
run-time.
%  we can show that finding the highest-scoring compression can be found in much more efficiently.


% The rest of the work will focus on 





% Define the score of a dependency arc as

% \[\theta(\uparray, h,m ) = w \cdot \phi(s, h, m)  \]

% maybe the bigram scores come from a language model.

% \[\omega(i,j) = \log p(s_i, s_j) \]



\section{Decoding}


We now present the decoding algorithm for sentence compression. We begin by
reviewing the standard dynamic programming algorithm for projective
dependency parsing, and then give an extension for sentence 
compression.

\subsection{Projective Parsing}

The standard decoding algorithm for projective dependency parsing is
known as Eisner's algorithm \cite{eisner, mcdonald}.  The algorithm is
specified through a set of deductive rules acting on items.  Each item
consists of a tuple $(t, i, j)$ where $i, j$ is a span
$0\leq i \leq j \leq n$ and $t$ is symbol from the set
$\{\ltri,\rtri, \ltrap, \rtrap, \abox\}$ (informally referred to as triangles, trapezoids, and boxes). These items indicate partial structures that may be 
combined with logical rules. For instance, the rules

\begin{center}
  \begin{tabular}{ll}
    \infer[S_z(i,j)]{(\rtrap, i,j)}{(\rtri, i,k)  &  (\ltri, k+1, j) }  & $\forall\   i\leq k < j$  \\\\
  \end{tabular}
\end{center}

\noindent combine right- and left-facing triangles covering $i,k$ and $k+1,j$ respectively to produce
a right-facing trapezoids covering $i,j$. 
The full set of $O(n^3)$ rules is given in Figure~\ref{fig:rules}.

% The goal item is a right-facing triangle $\rtri$ spanning the entire sentence $0,n$. 

A deductive rule may also have an associated \textit{consequence}
notated on its right, for instance the consequence of the above rule
is $S_z(i,j)$.  The consequence is a member of a semiring,
specified by a tuple $({\mathbb S},\oplus, \otimes, 0, 1)$ where
${\mathbb S}$ is a set of possible values, $\oplus$ and $\otimes$ are binary operators, and
0 and 1 are the annihilator and identity respectively. 
Define $S_z: {\IndexSetB} \mapsto {\mathbb S}$
 to map from dependency arcs to semiring values.

 We can combine items and semiring values using dynamic programming.
 To find the score of the highest-scoring parse, we can use the
 max-semiring $({\mathbb R},\max, +, -\infty, 0)$ and let $S_z(i,j) =
 \omega_{ij}$. This specification can be used to construct an $O(n^3)$
 CKY-style dynamic programming algorithm for dependency parsing. For other
 semirings, we can use this algorithm to find the semiring value of the goal
 item in $O((on)^3)$ time, where $o$ is the max cost of the $\otimes$ and
 $\oplus$ operator.

 
% The consequence of $$


% The
% main idea is quite simple. We extend the standard Eisner parsing
% algorithm to allow each premises to ``skip'' words.


\subsection{Compressive Parsing}

Next we turn to projective parsing that allows non-spanning trees and
incorporates a language model, we refer to this technique as
compressive parsing.

% From a high-level, the goal is to incorporate bigram-transitions into
% the parsing algorithm. Naively, this could be done by adding a predictive to
% each deductive rule.  An item $(t, i, j, j')$ would indicate 
% that $j'$ 

% However, this ignores the shared information
% between the two structures and results in an inefficient $O(n^6)$
% algorithm.

First, define a new function $S_y: {\IndexSet} \mapsto {\mathbb S}$ to
map word bigrams to semiring values. For the max-semiring this
function is $ S_y(i,j) = \theta_{ij} $. For no-deletion 
parsing, we can incorporate this new value directly into the
modification rules, e.g.

\begin{center}
  \begin{tabular}{ll}
    \infer[S_z(i,j) \otimes S_y(k, k+1)]{(\rtrap, i,j)}{(\rtri, i,k)  &  (\ltri, k+1, j) } \\
  \end{tabular}
\end{center}

\noindent For compressive parsing, however, we also want to merge items that may
have gaps, i.e. in-between words that are not descendants of either
item. This requires introducing rules that skip intermediate words. These 
rules can simply ignore the intermediate words i.e. for
all $ 0 \leq i\leq k < l < j \leq n$


\begin{center}
  \begin{tabular}{ll}{}
  \infer[S_z(i,j)\otimes S_y(k, l)]{(\rtrap, i,j)}{(\rtri, i,k)  &  (\ltri, l, j) } 
\end{tabular}
\end{center}

% An alternative approach is to simple allow non-adjacent items to
% combine in the dynamic program. For example, we can modify the
% right-facing dependency rule to allow an additional free variable
% $l$, 

\noindent These rules produce an $(i,j)$ arc, leave out words $j+1$
through $l-1$, and produce a $(k,l)$ bigram.  

Unfortunately this
modification results in an additional free variable $l$ and gives an
$O(n^4)$ parsing algorithm. Observe though that it is not necessary to produce the arc and bigram
with the same rule. In fact, we can force $k$ to predict if there will be
gap from $k+1$ to $l-1$ before applying other rules, and then apply the standard Eisner's rules, as if $k+1$ through $l-1$ were descendants of 
$k$ (and thus $i$ as well).  

This optimization is known as the ``hook trick'' \cite{}. 
We implement this trick by replacing initial items $(\rtri, i, i)$
with new items $(\rtriskip, i, i)$. These special items are only allowed 
to skip words to their right before becoming standard $\rtri$ items
 % indicates a right-facing item at $i$ that has no children yet and has skipped words $i+1$ through $j$.

  \begin{center}
    \begin{tabular}{ll}
      \infer{(\rtriskip, i,j)}{(\rtriskip, i, i)} &  $\forall\  0 \leq i < j \leq n$\\\\
      \infer[S_y(i,j+1)]{(\rtri, i,j)}{(\rtriskip, i, j)} & $ \forall\ 0 \leq i \leq j \leq  n$ \\\\
    \end{tabular}
  \end{center}

\noindent After skipping words $i+1$ to $j$ we incorporate the bigram consequence $S_y(i, j+1)$.

These hook rules allow us to avoid the extra free variable and maintain the same efficiency as Eisner's algorithm.
A full parse derivation is show in Figure~\ref{fig:parse}.

% Naively, this could be done by intersecting
% the dynamic program with a bigram lattice.  This intersection would
% yield items of the form $[t, i, j, i', j']$ where $i' < j'$ are the
% entering and leaving states of the lattice at each item. However, this
% yields a $O(n^6)$ algorithm for this task. 

% 


% \begin{center}
%   \infer[y_{i,j+1}]{(\rtri, i,j)}{(\rtriskip, i, j)} & $ \forall\ 0
%   \leq i \leq j \leq n$
% \end{center}



% Instead we make the observation that

% Define a new right-facing symbol $\rtriskip$.  

% \begin{center}
%   \infer[y_{i,j+1}]{(\rtri, i,j)}{(\rtriskip, i, j)} & $ \forall\ 0
%   \leq i \leq j \leq n$
% \end{center}

% The idea is to 


% % Instead we apply the ``hook trick'' to first.

% % The main addition to the algorithm is an additional right-facing symbol $\rtriskip$.


% Each item for standard dependency parsing consists of a tuple $[t, i, j]$ where $t$ is a symbol in $\{\ltri,\rtri, \ltrap, \rtrap, \rtriskip \}$  and $0\leq i \leq j \leq n$.

% We extend the first order model to skip parsing to additionally score the bigrams chosen in the final parse. To do this we extend the item definition to include the anticipated next word $(t, i, j)$.

% The only new addition is that we use the ``hook trick'' to select the best next word for each premise item before using it. The other rules are all identical.

% Note that for this to work, it is crucial that we only allow skipping words on the right and that the left side index $i$ is always the left-most word used in the item.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{../../notebooks/diagrams}
  \label{fig:parse}
  \caption{Example of a compressive parse with derivation. The compressed sentence is ``Production closed for Christmas''. The output structure has arc and bigrams $y_{0,3}, y_{3,1},y_{3,7},y_{7,9},z_{0,1},z_{1,3},z_{3,7}, z_{7,9},$ and $z_{9,11}$ set to 1 and all others set to 0. The full first-order derivation tree is shown below. The final derivation has $||z||_1=4$. }
\end{figure*}


\subsection{Second-Order Compressive Parsing}

A similar algorithm can be used for second-order dependency parsing \cite{}. For a second-order parser
we replace the index set $\IndexSet$ with a new index set that also includes a \textit{sibling}
index $j$ (the previous modifier on the same side as modifier $k$). 

\[ \IndexSetB=  \begin{cases} (i,j,k) : 
    i \in \EnumS{0}{n}, j \in \Enum{n}, \\
    \ \ \ \ \  (i < k < j) \lor (i >k  > j) \lor k = \NULL
  \end{cases} \ \]
\noindent where $\NULL$ indicates that $k$ is the first modifier on its side. 

The deductive rules for second-order parsing are shown in
Figure~\ref{fig:rules}. They result in an $O(n^3)$ parsing algorithm.

The main complication for second-order compressive parsing is the rule 

\[  \infer[S_z(i,\epsilon,j)]{(\rtrap, i,j)}{(\rtri, i,i)  &  (\ltri, i+1, j) } \ \  \forall i < j\]

\noindent which creates the initial right-modifier of $i$, $(i, \NULL, j)$. This rule
implicitly assumes that all items $(\rtri, i, j)$ with $i\neq j$ have
already taken at least one modifier. However, with the new rules, it is possible 
that $(\rtri, i, j)$ may have skipped $i+1$ through $j$. Therefore it is necessary 
to also allow these items to take a first right-modifier as well, i.e.

% % which is broken by the hook trick. 
% For compressive parsing, this rule also should accept items that 
% have skipped words but have not yet taken a modifier

\[  \infer[S_z(i,\NULL, j)]{(\rtrap, i,j)}{(\rtriskip, i,k)  &  (\ltri, k+1, j) } \ \  \forall i \leq k < j \]

The remaining second-order rules are identical parsing, and
second-order compressive CKY can be run in $O(n^3)$ time. The full set of rules is show in
Figure~\ref{fig:rules}.


\section{Compression Limits}
\label{sec:comprate}
Sentence compression problems often have a target range for the
compressed sentence. That is we might have upper and lower bounds 
for the size $m$ of the final sentence, i.e.

\begin{eqnarray*}
  q(m, \theta, \omega) = &&\max_{y \in {\cal Y}, z\in {\cal Z}}  \theta^\top y +  \omega^\top z  \\
  \mathrm{s .t}&&  \sum_{i = 0}^{j-1} y_{i j} =  \sum_{i=0}^n z_{i j} \ \ \forall\  1 \leq j \leq n, \\
  && ||z ||_1 = m
\end{eqnarray*}

In this section we discuss two methods for enforcing these limits, 
first by dynamic programming and the second by bisection.

\subsection{Resource-Constrained Parsing}

Resource-constrained problems require finding an optimal 
solution under a resource limit or requirement. Without loss
of generality we will assume that a resource begins at $0$ 
and is upper-bounded by $d$. We can modify a dynamic program
to keep track of the amount of resources consumed.

To track resources, define the resource-constrained max-semiring as 

\[(\Reals^{\{0\ldots d\}}, \max_{rc}, +_{rc}, -\mathbf{\infty}, \mathbf{1})\]

\noindent where for an item with value $r \in \Reals^d$ , $r_i$ is the
highest-score seen using $i$ resources, and we define
\begin{eqnarray*}
 \max_{rc} \{r, s\} &=& \sum_{i = 0}^d \delta_i \max \{r_i, s_i\}  \\
 r +_{rc} s &=& \sum_{i = 0}^d \delta_i  \max_{j ,k: j+k =i}  r_j + s_k   
\end{eqnarray*}


For this semiring $\otimes$ has runtime $O(d^2)$. The runtime for CKY-style
projective parsing is therefore $O(n^3d^2)$.


For sentence compression, one resource of interest would be the number of 
words kept. The upper bound for this resource is $d=m$, and the semirings 
at each item would be

\[ S_y(i,j) = \delta_0  \theta_{ij} , \ \  S_z(i,j) = \delta_1 \omega_{ij}  \]

If the goal item has value is $r^*$ then $q(m) = r^*_m$.

Alternatively when $n - m < m$ it makes sense to reverse the resource to 
count words dropped. We can set $d = n - m$ and 

\[S_y(i,j) = \delta_{j-i-1}  \theta_{ij} ,\ \   S_z(i,j) = \delta_0 \omega_{ij}    \]

Picking the better of these resources gives a worst-case run-time of $O(n^3\min\{m,n-m\}^2)$. 

% This extra requirement can be represented by a single constraint. However in practice it is difficult to incorporate this into the dynamic programming algorithm. One idea is to intersect with a counting FSA. In general each item may span $d$ vertices with $0 \leq d \leq m$, and each sub-item may span $d'$ and $d - d'$ vertices respectively, i.e.
% \[
% \infer{(\rtri, i,j, d)}{(\rtrap, i,k, d')  & (\rtri, k, j, d) }    \  \forall i< k \leq j, 0\leq d' < d \leq < m_2  \\\\
% \]

% This intersection adds a factor of $m^2$ to the running time, which makes the full algorithm $O(n^5)$.




% \subsection{Compression by Bisection}

% An alternative approach for single-resource constraints is to  
% relax the constraint and 


% \begin{eqnarray*}
%   L(\lambda, y, z) = && \theta^\top y +  \omega^\top z + \lambda ||z||_1   \\
% \end{eqnarray*}

% For any fixed value of $\lambda$ we can calculate $y_\lambda, z_\lambda = \argmax_{y, z} L(\lambda, y, z)$ using the dynamic programming algorithm shown. 

% \[ S_y(i,j) =  \theta_ij  + \lambda \]

% %We simply replace $\theta$ with $\theta'$ where $\theta'(i,j) = \theta(i,j) + \lambda$ for all $(i,j) \in \IndexSet$.

% By weak duality $\theta^\top y_\lambda + \omega^\top z_\lambda$ will always give an upper bound on the optimal constrained solution $y^*, z^*$. And if $||z_\lambda||_1 = m$ then this upper bound is provably tight.
% Past work in NLP has looked at minimizing this dual upper bound with subgradient descent; however we can exploit the fact that there is only a single dual variable $\lambda$ and use a more efficient method.


% We will minimize $L(\lambda)$ using the bisection method. First note that
% % with $m=n$ we can solve $q(m)$ by setting $\lambda$ to the max bigram score  , similarly
% % with $m=0$ we can solve $q(0)$ by setting $\lambda$ to the inverse of the max bigram score.


% The bisection method is guaranteed to find an optimal solution if one exists in  iterations
% % \todo[inline]{Figure this result out.}


% \begin{figure}
%   \begin{algorithmic}
%     \Procedure{Bisection}{$m \lambda^{(\uparrow)}, \lambda^{(\downarrow)}$}
%     % \State{$\lambda^{(\uparrow)} \gets \max_{(i, j) \in \IndexSetB} \omega(i,j)$}
%     % \State{$\lambda^{(\downarrow)} \gets -\max_{(i, j) \in \IndexSetB} \omega(i,j)$}
%     \State{$\lambda^{(0)} \gets \frac{\lambda^{(\uparrow)} + \lambda^{(\downarrow)}}{2}$}
%     \For{$k = 1\hbox{ to }K$}
%     \State{ $y^{(k)}, z^{(k)} = \argmax_{y, z} L(\lambda^{(k-1)}, y, z)$}
%     \If{$  ||z^{(k)}||_1 = m$}
%     \State{\Return{$y^{(k)}, z^{(k)}$}}
%     \EndIf{}
%     \If{$||z^{(k)}||_1 < m$}$ \ \ \lambda^{(\uparrow)} \gets \lambda^{(k)}$
%     \Else$\ \ \lambda^{(\downarrow)} \gets \lambda^{(k)}$ 
%     \EndIf{}
%     \State{$\lambda^{(k+1)} \gets \frac{\lambda^{(\uparrow)} + \lambda^{(\downarrow)}}{2}$}
%     \EndFor{}
%     \EndProcedure{}
%   \end{algorithmic}
% \end{figure}



\begin{figure*}
  \noindent \textbf{Premises:}
  \[(\ltri, i,i), (\rtri, i,i)\ \ \forall\ 0 \leq i \leq j \leq  n\]

\noindent \textbf{Goal:}\[ (\rtri, 0,n)\]

\noindent \textbf{Completion Rules:}

\begin{eqnarray*}
  \infer{(\rtri, i,j)}{(\rtrap, i,k)  &  (\rtri, k, j) }    &  \forall\ 0 \leq i<  k \leq j \leq n\\\\
  \infer{(\ltri, i,j)}{(\ltri, i,k)  &  (\ltrap, k, j) }  & \forall\ 0 < i\leq k < j \leq n\\\\
\end{eqnarray*}

\begin{multicols}{2}


\noindent \textbf{First-Order Rules:}
\air \air

\begin{tabular}{ll}
  \infer[S_z(i,j)]{(\rtrap, i,j)}{(\rtri, i,k)  &  (\ltri, k+1, j) }  & $\forall\   i\leq k < j$  \\\\
  \infer[S_z(j, i)]{(\ltrap, i,j)}{(\rtri, i,k)  &  (\ltri, k+1, j) } & $\forall\ i\leq k < j $\\\\
  % \infer{(\rtrapskip, i,j, p)}{(\rtriskip, i,k-1, p)  &  (\ltri, k, k, k) } &  \forall i < p\\\\
  % \infer{(\rtriskip, i,k, p)}{(\rtrapskip, i,k, p)  &  (\rtri, k, k, k) } &  \forall i < k < p\\\\
\end{tabular}


% \begin{figure}

%   \noindent \textbf{Premises:}
%   \[(\ltri, i,i), (\rtriskip, i,i)\ \ \forall\ 0 \leq i \leq j \leq  n\]

% \noindent \textbf{Goal:}\[ (\rtri, 0,n)\]
\columnbreak
  \noindent \textbf{Second-Order Rules:}
\air \air

\begin{tabular}{ll}
  % \infer[\mathrm{Bigram}(i,p)]{(\rtriskip, i,i, p)}{(\rtriskip, i, i,i)} &  \forall  0 \leq i < p \leq n + 1\\\\
  \infer{(\abox, i,j)}{(\rtri, i,k)  &  (\ltri, k+1, j) } &  $\forall\ i\leq k < j $ \\\\
  \infer[S_z(i, \NULL, j)]{(\rtrap, i,j)}{(\rtri, i,i)  &  (\ltri, i+1, j) } &  $\forall\  i < j $\\\\
  \infer[S_z(j, \NULL, i)]{(\ltrap, i,j)}{(\rtri, i,j-1)  &  (\ltri, j, j) } & $\forall\   i < j $\\\\
  \infer[S_z(i, k, j)]{(\rtrap, i,j)}{(\rtrap, i,k)  &  (\abox, k, j) } &  $\forall\ i\leq k < j$ \\\\
  \infer[S_z(j,  k, i)]{(\ltrap, i,j)}{(\abox, i,k)  &  (\ltrap, k, j) } &  $\forall\ i\leq k < j$\\\\
  % \infer{(\rtri, i,j)}{(\rtrap, i,k)  &  (\rtri, k, j) }    &  \forall\ 0 \leq i <  k \leq j \leq n \\\\
  % \infer{(\ltri, i,j)}{(\ltri, i,k)  &  (\ltrap, k, j) }  & \forall\ 0< i\leq k < j\leq n \\\\
\end{tabular}
% \textbf{Additional Rules:}

% \begin{eqnarray*}
%   % \infer[z(i,j+1)]{(\rtri, i,j)}{(\rtriskip, i, j)} &  \forall\ 0 \leq i \leq j\leq n\\\\
%   % \infer{(\rtriskip, i,j)}{(\rtriskip, i, i)} &  \forall\ 0 \leq i < j \leq n\\\\
%   \infer[y(i, \NULL, j)]{(\rtrap, i,j)}{(\rtriskip, i,k)  &  (\ltri, k+1, j) } &  \forall\ 0 \leq i < j \leq n\\\\
% \end{eqnarray*}
\end{multicols}

\hline
\air 

  \noindent \textbf{Revised Premises:}
  \[(\ltri, i,i), (\rtriskip, i,i)\ \ \forall\ 0 \leq i \leq j \leq  n\]
  
  \noindent \textbf{Additional Rules:}
  \air \air 

  \begin{center}
    \begin{tabular}{ll}
      \infer{(\rtriskip, i,j)}{(\rtriskip, i, i)} &  $\forall\  0 \leq i < j \leq n$\\\\
      \infer[S_y(i,j+1)]{(\rtri, i,j)}{(\rtriskip, i, j)} & $ \forall\ 0 \leq i \leq j \leq  n$ \\\\
      \infer[S_y(i,k + 1)\otimes S_z(i, \NULL, j)]{(\rtrap, i,j)}{(\rtriskip, i,k)  &  (\ltri, k+1, j) } &  $\forall\ 0 \leq i<  k <  j \leq n $ \\
      & [Second-Order only]
    \end{tabular}
  \end{center}
 
\caption{Deductive rules for standard dependency parsing and the extensions for compressive dependency parsing. First- and second-order parsing share the same premises, goal, and a set of completion rules. Compressive parsing has a revised set of premises and extends the standard rules with three additional types. $S_z$ and $S_y$ are functions mapping arcs and bigrams, respectively, to semiring values. }
\label{fig:rules}
\end{figure*} 

\section{Features}
\subsection{Features and Learning}
\label{features}

We based the features for our discriminative model on the features
described by \newcite{mcdonald06a} and \newcite{thadani13a}.

Dependency features $\Omega$ included the probability of an arc under
a smoothed dependency grammar derived from the Penn Treebank, the
\emph{fidelity} of the output arc---an indicator of whether it is present in
the input parse---conjoined with its dependency labels, and conjunctions
of the following features: arc fidelity, fidelity of ancestral relations
binned by the distance of the ancestor, direction of attachment,
part-of-speech (POS) tags of the tokens incident to the edge, labels of
constituent chunks containing these tokens and an indicator of whether
they lie in the same chunk.

We also added token-specific features over the dependents of each
arc. These included POS tag sequences of length at most 3 around the token,
the label of the dependency arc to the token in the
input parse conjoined with its POS tag, indicators for
capitalized words and words in parentheses as well as lexical features
covering verb stems, symbols and negations.

Second-order features were comprised of the fidelity of the
second-order dependency conjoined with its label in the input parse.
Although we experimented with a wide range of second-order features on
a development dataset, we found that they were prone to overfitting,
likely due to the small size of the \newcite{clarke08}
datasets used in our evaluation.\footnote{We
    expect these features to be valuable when training on larger
    compression datasets such as the one proposed by
    \newcite{filippova13}.}

Finally, bigram features $\theta$ include the likelihood under a
language model (LM) contructed from the Gigaword corpus, the coarse and
fine POS tags of its tokens and the labels of dependency arcs incident to
these words in the input sentence.

For the experiments in the following section, we trained models using
a variant of the structured perceptron \cite{collins02}
which incorporates minibatches~\cite{zhao13} for easy parallelization and
faster convergence.\footnote{
    We used a minibatch size of 4 in all experiments.}
Overfitting was avoided by averaging parameters and monitoring performance
against a held-out development set during training.
%We followed \newcite{martins09c} in using LP-relaxations
%when training models with ILP inference,
%assuming algorithmic separability~\cite{kulesza07}
%for these compression problems.

\section{Experiments}
\label{experiments}

\section{Experiments}
\label{experiments}

This section describes the experimental setup used to evaluate the
utility and performance of our proposed inference approach.

\subsection{Datasets}
\label{data}
Compression experiments were conducted over the standard newswire (NW) and
broadcast news transcription (BN) corpora compiled by \newcite{clarke08}.
These datasets include gold compressions which were produced by human
annotators
%---3 in the case of the BN corpus---
who were restricted to only
delete tokens.
We filtered the instances as per \newcite{thadani13a} and used the
same training/dev/test splits as \newcite{clarke08},
resulting in 953/63/603 instances
for the NW corpus and 880/78/404 for the BN corpus.
Reference dependency parses were approximated by parsing reference
compressions with the Stanford dependency
parser.\footnote{\texttt{http://nlp.stanford.edu/software/}}
Following \newcite{napoles11a}, which revealed a strong correlation between
system compression rate and human judgments of compression quality,
all systems were constrained to produce compressed output at a fixed
compression rate determined by the the median reference compressions
for each instance.

\subsection{Evaluation measures}
\label{measures}
We evaluated system performance using F$_1$ metrics over n-grams and
dependencies---produced by parsing
system output with
RASP~\cite{briscoe06} and the Stanford parser---following previous
work~\cite{unno06,clarke08,martins09a,napoles11a,thadani13a,thadani14}.
In the case of the BN corpus which has 3 reference compressions per instance,
F$_1$ scores were averaged over all references.
%, which is frequently used in compression evaluations.
%All ILPs were solved using
%Gurobi,\footnote{\texttt{http://www.gurobi.com}} a high-performance
%commercial-grade solver.
%---to ensure that the
%reported differences between the systems under study are meaningful.



% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{full.bib}

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

